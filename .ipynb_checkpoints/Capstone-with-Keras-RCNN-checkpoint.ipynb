{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from glob import glob # module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\n",
    "import os\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting our initial parameters and pathnames\n",
    "dsb_data_dir = os.path.join('./', 'Datasets')\n",
    "stage_label = 'stage1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training labels dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14905</th>\n",
       "      <td>7978812d0e2e034ee1f9c141f019705582fcaa290e4a01...</td>\n",
       "      <td>[36001, 2, 36361, 3, 36721, 4, 37081, 5, 37441...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28282</th>\n",
       "      <td>f29fd9c52e04403cd2c7d43b6fe2479292e53b2f61969d...</td>\n",
       "      <td>[567094, 4, 567696, 6, 568298, 8, 568900, 9, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20567</th>\n",
       "      <td>aaa52100fafaa50877e777229cdf6cde7c422f145ff671...</td>\n",
       "      <td>[58552, 21, 59070, 26, 59588, 31, 60107, 34, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ImageId  \\\n",
       "14905  7978812d0e2e034ee1f9c141f019705582fcaa290e4a01...   \n",
       "28282  f29fd9c52e04403cd2c7d43b6fe2479292e53b2f61969d...   \n",
       "20567  aaa52100fafaa50877e777229cdf6cde7c422f145ff671...   \n",
       "\n",
       "                                           EncodedPixels  \n",
       "14905  [36001, 2, 36361, 3, 36721, 4, 37081, 5, 37441...  \n",
       "28282  [567094, 4, 567696, 6, 568298, 8, 568900, 9, 5...  \n",
       "20567  [58552, 21, 59070, 26, 59588, 31, 60107, 34, 6...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv(os.path.join(dsb_data_dir,'{}_train_labels.csv'.format(stage_label)))\n",
    "train_labels['EncodedPixels'] = train_labels['EncodedPixels'].map(lambda ep: [int(x) for x in ep.split(' ')])\n",
    "train_labels.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in all Images\n",
    "Here we load in the images and process the paths so we have the appropriate information for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>ImageId</th>\n",
       "      <th>ImageType</th>\n",
       "      <th>TrainingSplit</th>\n",
       "      <th>Stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>./Datasets/stage1_train/76faaed50ed6ea6814ac36...</td>\n",
       "      <td>76faaed50ed6ea6814ac36199964b86fb09ba7f41a6f21...</td>\n",
       "      <td>masks</td>\n",
       "      <td>train</td>\n",
       "      <td>stage1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>./Datasets/stage1_train/ba3997edd3fcb2f823ecdf...</td>\n",
       "      <td>ba3997edd3fcb2f823ecdf870d2b607f08bff848f72a5c...</td>\n",
       "      <td>masks</td>\n",
       "      <td>train</td>\n",
       "      <td>stage1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  \\\n",
       "15003  ./Datasets/stage1_train/76faaed50ed6ea6814ac36...   \n",
       "22512  ./Datasets/stage1_train/ba3997edd3fcb2f823ecdf...   \n",
       "\n",
       "                                                 ImageId ImageType  \\\n",
       "15003  76faaed50ed6ea6814ac36199964b86fb09ba7f41a6f21...     masks   \n",
       "22512  ba3997edd3fcb2f823ecdf870d2b607f08bff848f72a5c...     masks   \n",
       "\n",
       "      TrainingSplit   Stage  \n",
       "15003         train  stage1  \n",
       "22512         train  stage1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images = glob(os.path.join(dsb_data_dir, 'stage1_*', '*', '*', '*'))\n",
    "img_df = pd.DataFrame({'path': all_images})\n",
    "\n",
    "img_id = lambda in_path: in_path.split('/')[-3]\n",
    "img_type = lambda in_path: in_path.split('/')[-2]\n",
    "img_group = lambda in_path: in_path.split('/')[-4].split('_')[1]\n",
    "img_stage = lambda in_path: in_path.split('/')[-4].split('_')[0]\n",
    "\n",
    "img_df['ImageId'] = img_df['path'].map(img_id)\n",
    "img_df['ImageType'] = img_df['path'].map(img_type)\n",
    "img_df['TrainingSplit'] = img_df['path'].map(img_group)\n",
    "img_df['Stage'] = img_df['path'].map(img_stage)\n",
    "\n",
    "img_df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data\n",
    "Here we make training data and load all the images into the dataframe. We take a simplification here of grouping all the regions together (rather than keeping them distinct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df = img_df.query('TrainingSplit==\"train\"')\n",
    "train_rows = []\n",
    "group_cols = ['Stage', 'ImageId']\n",
    "\n",
    "for n_group, n_rows in train_df.groupby(group_cols):\n",
    "    c_row = {col_name: col_value for col_name, col_value in zip(group_cols, n_group)}\n",
    "    c_row['mask_paths'] = n_rows.query('ImageType == \"masks\"')['path'].values.tolist()\n",
    "    c_row['image_paths'] = n_rows.query('ImageType == \"images\"')['path'].values.tolist()\n",
    "    \n",
    "    train_rows += [c_row]\n",
    "train_img_df = pd.DataFrame(train_rows)    \n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "def read_and_stack(in_img_list):\n",
    "    return np.sum(np.stack([imread(c_img) for c_img in in_img_list], 0), 0)/255.0\n",
    "train_img_df['images'] = train_img_df['image_paths'].map(read_and_stack).map(lambda x: x[:,:,:IMG_CHANNELS])\n",
    "\n",
    "def read_and_stack_masks(in_img_list):\n",
    "    return np.sum(np.stack([i*(imread(c_img)>0) for i, c_img in \n",
    "                            enumerate(in_img_list,1)], 0), 0)\n",
    "train_img_df['masks'] = train_img_df['mask_paths'].map(read_and_stack_masks).map(lambda x: x.astype(int))\n",
    "train_img_df.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a few images\n",
    "Here we show a few images of the cells where we see there is a mixture of brightfield and fluorescence which will probably make using a single segmentation algorithm difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_img = 6\n",
    "fig, m_axs = plt.subplots(2, n_img, figsize = (12, 4))\n",
    "for (_, c_row), (c_im, c_lab) in zip(train_img_df.sample(n_img).iterrows(), \n",
    "                                     m_axs.T):\n",
    "    c_im.imshow(c_row['images'])\n",
    "    c_im.axis('off')\n",
    "    c_im.set_title('Microscope')\n",
    "    \n",
    "    c_lab.imshow(c_row['masks'])\n",
    "    c_lab.axis('off')\n",
    "    c_lab.set_title('Labeled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Dimensions\n",
    "Here we show the dimensions of the data to see the variety in the input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_df['images'].map(lambda x: x.shape).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_df.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a fancy dictionary with bounding boxes\n",
    "Reformat the data for bounding box style annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import label\n",
    "from skimage.measure import regionprops\n",
    "train_dict = [dict(image = dict(\n",
    "    pathname = c_row['image_paths'][0],\n",
    "                  shape =  dict(zip(['r', 'c', 'channels'], c_row['images'].shape))),\n",
    "                   objects = [{'bounding_box' : dict(\n",
    "                    minimum = dict(r = c_reg.bbox[0], c = c_reg.bbox[1]),\n",
    "                   maximum = dict(r = c_reg.bbox[2], c = c_reg.bbox[3])),\n",
    "                                  'class' : \"nucleus\",\n",
    "                               'mask': dict(pathname = c_path)\n",
    "                              }\n",
    "                                   for c_reg, c_path in zip(\n",
    "                                       regionprops(label(c_row['masks'])),\n",
    "                                       c_row['mask_paths']\n",
    "                                   )]\n",
    "                  ) \n",
    "              for _, c_row in train_img_df.iterrows()]\n",
    "print(str(train_dict[0])[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Improved Data Generator\n",
    "I stole this nice generator from the latest pull request on the rcnn repo, thanks Allen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend\n",
    "import keras.preprocessing.image\n",
    "import numpy\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "\n",
    "def scale_size(size, min_size, max_size):\n",
    "    \"\"\"\n",
    "    Rescales a given image size such that the larger axis is\n",
    "    no larger than max_size and the smallest axis is as close\n",
    "    as possible to min_size.\n",
    "    \"\"\"\n",
    "    assert (len(size) == 2)\n",
    "\n",
    "    scale = min_size / numpy.min(size)\n",
    "\n",
    "    # Prevent the biggest axis from being larger than max_size.\n",
    "    if numpy.round(scale * numpy.max(size)) > max_size:\n",
    "        scale = max_size / numpy.max(size)\n",
    "\n",
    "    rows, cols = size\n",
    "    rows *= scale\n",
    "    cols *= scale\n",
    "\n",
    "    return (int(rows), int(cols)), scale\n",
    "\n",
    "\n",
    "class DictionaryIterator(keras.preprocessing.image.Iterator):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dictionary,\n",
    "            classes,\n",
    "            generator,\n",
    "            target_shape=None,\n",
    "            scale=1,\n",
    "            ox=None,\n",
    "            oy=None,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            seed=None\n",
    "    ):\n",
    "        self.dictionary = dictionary\n",
    "        self.classes = classes\n",
    "        self.generator = generator\n",
    "\n",
    "        r = dictionary[0][\"image\"][\"shape\"][\"r\"]\n",
    "        c = dictionary[0][\"image\"][\"shape\"][\"c\"]\n",
    "\n",
    "        channels = dictionary[0][\"image\"][\"shape\"][\"channels\"]\n",
    "\n",
    "        self.image_shape = (r, c, channels)\n",
    "        self.scale = scale\n",
    "        self.ox = ox\n",
    "        self.oy = oy\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if target_shape is None:\n",
    "            self.target_shape, self.scale = scale_size(self.image_shape[0:2], numpy.min(self.image_shape[:2]), numpy.max(self.image_shape[:2]))\n",
    "\n",
    "            self.target_shape = self.target_shape + (self.image_shape[2],)\n",
    "\n",
    "        else:\n",
    "            self.target_shape = target_shape + (self.image_shape[2],)\n",
    "\n",
    "        # Metadata needs to be computed only once.\n",
    "        r, c, channels = self.target_shape\n",
    "\n",
    "        self.target_metadata = numpy.array([[r, c, self.scale]])\n",
    "\n",
    "        super(DictionaryIterator, self).__init__(len(self.dictionary), batch_size, shuffle, seed)\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            selection = next(self.index_generator)\n",
    "\n",
    "        return self._get_batches_of_transformed_samples(selection)\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, selection):\n",
    "        # Labels has num_classes + 1 elements, since 0 is reserved for\n",
    "        # background.\n",
    "        num_classes = len(self.classes)\n",
    "\n",
    "        target_bounding_boxes = numpy.zeros((self.batch_size, 0, 4), dtype=keras.backend.floatx())\n",
    "\n",
    "        target_images = numpy.zeros((self.batch_size,) + self.target_shape, dtype=keras.backend.floatx())\n",
    "\n",
    "        target_masks = numpy.zeros((self.batch_size,) + self.target_shape, dtype=keras.backend.floatx())\n",
    "\n",
    "        target_scores = numpy.zeros((self.batch_size, 0, num_classes + 1), dtype=numpy.uint8)\n",
    "\n",
    "        for batch_index, image_index in enumerate(selection):\n",
    "            count = 0\n",
    "\n",
    "            while count == 0:\n",
    "                # Image\n",
    "                target_image_pathname = self.dictionary[image_index][\"image\"][\"pathname\"]\n",
    "\n",
    "                target_image = skimage.io.imread(target_image_pathname)[:, :, :3]\n",
    "\n",
    "                if target_image.ndim == 2:\n",
    "                    target_image = numpy.expand_dims(target_image, -1)\n",
    "\n",
    "                # crop\n",
    "                if self.ox is None:\n",
    "                    offset_x = numpy.random.randint(0, self.image_shape[1] - self.target_shape[1] + 1)\n",
    "                else:\n",
    "                    offset_x = self.ox\n",
    "\n",
    "                if self.oy is None:\n",
    "                    offset_y = numpy.random.randint(0, self.image_shape[0] - self.target_shape[0] + 1)\n",
    "                else:\n",
    "                    offset_y = self.oy\n",
    "\n",
    "                target_image = target_image[offset_y:self.target_shape[0] + offset_y, offset_x:self.target_shape[1] + offset_x, :]\n",
    "\n",
    "                # Copy image to batch blob.\n",
    "                target_images[batch_index] = skimage.transform.rescale(target_image, scale=self.scale, mode=\"reflect\")\n",
    "\n",
    "                # Set ground truth boxes.\n",
    "                for object_index, b in enumerate(self.dictionary[image_index][\"objects\"]):\n",
    "                    if b[\"class\"] not in self.classes:\n",
    "                        continue\n",
    "\n",
    "                    bounding_box = b[\"bounding_box\"]\n",
    "\n",
    "                    minimum_c = bounding_box[\"minimum\"][\"c\"] - offset_x\n",
    "                    minimum_r = bounding_box[\"minimum\"][\"r\"] - offset_y\n",
    "\n",
    "                    maximum_c = bounding_box[\"maximum\"][\"c\"] - offset_x\n",
    "                    maximum_r = bounding_box[\"maximum\"][\"r\"] - offset_y\n",
    "\n",
    "                    if maximum_c == target_image.shape[1]:\n",
    "                        maximum_c -= 1\n",
    "\n",
    "                    if maximum_r == target_image.shape[0]:\n",
    "                        maximum_r -= 1\n",
    "\n",
    "                    if minimum_c >= 0 and maximum_c < target_image.shape[1] and minimum_r >= 0 and maximum_r < target_image.shape[0]:\n",
    "                        count += 1\n",
    "\n",
    "                        target_bounding_box = [\n",
    "                            minimum_c,\n",
    "                            minimum_r,\n",
    "                            maximum_c,\n",
    "                            maximum_r\n",
    "                        ]\n",
    "\n",
    "                        target_bounding_boxes = numpy.append(target_bounding_boxes, [[target_bounding_box]], axis=1)\n",
    "\n",
    "                        target_score = [0] * (num_classes + 1)\n",
    "\n",
    "                        target_score[self.classes[b[\"class\"]]] = 1\n",
    "\n",
    "                        target_scores = numpy.append(target_scores, [[target_score]], axis=1)\n",
    "\n",
    "            # Scale the ground truth boxes to the selected image scale.\n",
    "            target_bounding_boxes[batch_index, :, :4] *= self.scale\n",
    "\n",
    "        return [target_bounding_boxes, target_images, target_masks, self.target_metadata, target_scores], None\n",
    "\n",
    "\n",
    "class ImageSegmentationGenerator:\n",
    "    def flow(self, dictionary, classes, target_shape=None, scale=None, ox=None, oy=None, batch_size=1, shuffle=True, seed=None):\n",
    "        return DictionaryIterator(dictionary, classes, self, target_shape, scale, ox, oy, batch_size, shuffle, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see it's loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "seg_gen = ImageSegmentationGenerator().flow(train_dict, {'nucleus': 1})\n",
    "x_out, _ = seg_gen.next()\n",
    "print('x', len(x_out))\n",
    "for x in x_out:\n",
    "    print('\\t', x.shape)\n",
    "\n",
    "(target_bounding_boxes, target_image, _, _, target_scores) = x_out\n",
    "target_bounding_boxes = np.squeeze(target_bounding_boxes)\n",
    "target_image = np.squeeze(target_image)\n",
    "target_scores = np.argmax(target_scores, -1)\n",
    "target_scores = np.squeeze(target_scores)\n",
    "\n",
    "_, axis = plt.subplots(1, figsize=(12, 8))\n",
    "axis.imshow(target_image)\n",
    "\n",
    "for target_index, target_score in enumerate(target_scores):\n",
    "    if target_score > 0:\n",
    "        xy = [\n",
    "            target_bounding_boxes[target_index][0],\n",
    "            target_bounding_boxes[target_index][1]\n",
    "        ]\n",
    "\n",
    "        w = target_bounding_boxes[target_index][2] - target_bounding_boxes[target_index][0]\n",
    "        h = target_bounding_boxes[target_index][3] - target_bounding_boxes[target_index][1]\n",
    "\n",
    "        rectangle = matplotlib.patches.Rectangle(xy, w, h, \n",
    "                                                 edgecolor=plt.cm.RdBu(target_score), \n",
    "                                                 facecolor=\"none\")\n",
    "        axis.add_patch(rectangle)\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras_rcnn.models import RCNN\n",
    "img_in = Input((None, None, 3))\n",
    "rcnn_model = RCNN(img_in, 2)\n",
    "optimizer = keras.optimizers.Adam(0.0001)\n",
    "rcnn_model.compile(optimizer)\n",
    "print('Input:', rcnn_model.get_input_shape_at(0))\n",
    "print('Output:', rcnn_model.get_output_shape_at(0))\n",
    "#rcnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gen():\n",
    "    for (boxes, target_img, img_b, meta_out, target_scores), _ in seg_gen:\n",
    "        # for debugging\n",
    "        #print([(i, x.shape) for i, x in enumerate((boxes, target_img, img_b, meta_out, target_scores))])\n",
    "        yield [boxes, target_img, target_scores, meta_out], None\n",
    "rcnn_model.fit_generator(fit_gen(),\n",
    "                        steps_per_epoch = 20,\n",
    "                        epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Predictions on Single Image\n",
    "Here we apply the model to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = next(fit_gen())\n",
    "y_anchors, y_scores = rcnn_model.predict(test_ds[0])\n",
    "print(y_anchors.shape, y_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bounding_boxes = np.squeeze(y_anchors)\n",
    "target_image = np.squeeze(test_ds[0][1][0])\n",
    "target_scores = np.argmax(np.squeeze(y_scores), -1)\n",
    "\n",
    "_, axis = plt.subplots(1, figsize=(12, 8))\n",
    "\n",
    "axis.imshow(target_image)\n",
    "\n",
    "for target_index, target_score in enumerate(target_scores):\n",
    "    if target_score > 0:\n",
    "        xy = [\n",
    "            target_bounding_boxes[target_index][0],\n",
    "            target_bounding_boxes[target_index][1]\n",
    "        ]\n",
    "\n",
    "        w = target_bounding_boxes[target_index][2] - target_bounding_boxes[target_index][0]\n",
    "        h = target_bounding_boxes[target_index][3] - target_bounding_boxes[target_index][1]\n",
    "\n",
    "        rectangle = matplotlib.patches.Rectangle(xy, w, h, \n",
    "                                                 edgecolor=plt.cm.RdBu(target_score), \n",
    "                                                 facecolor=\"none\")\n",
    "        axis.add_patch(rectangle)\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_df = img_df.query('TrainingSplit==\"test\"')\n",
    "test_rows = []\n",
    "group_cols = ['Stage', 'ImageId']\n",
    "for n_group, n_rows in test_df.groupby(group_cols):\n",
    "    c_row = {col_name: col_value for col_name, col_value in zip(group_cols, n_group)}\n",
    "    c_row['images'] = n_rows.query('ImageType == \"images\"')['path'].values.tolist()\n",
    "    test_rows += [c_row]\n",
    "test_img_df = pd.DataFrame(test_rows)    \n",
    "\n",
    "test_img_df['images'] = test_img_df['images'].map(read_and_stack).map(lambda x: x[:,:,:IMG_CHANNELS])\n",
    "print(test_img_df.shape[0], 'images to process')\n",
    "test_img_df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_img_df['masks'] = test_img_df['images'].map(lambda x: simple_cnn.predict(np.expand_dims(x, 0))[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a few predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_img = 3\n",
    "from skimage.morphology import closing, opening, disk\n",
    "def clean_img(x):\n",
    "    return opening(closing(x, disk(1)), disk(3))\n",
    "fig, m_axs = plt.subplots(3, n_img, figsize = (12, 6))\n",
    "for (_, d_row), (c_im, c_lab, c_clean) in zip(test_img_df.sample(n_img).iterrows(), \n",
    "                                     m_axs):\n",
    "    c_im.imshow(d_row['images'])\n",
    "    c_im.axis('off')\n",
    "    c_im.set_title('Microscope')\n",
    "    \n",
    "    c_lab.imshow(d_row['masks'])\n",
    "    c_lab.axis('off')\n",
    "    c_lab.set_title('Predicted')\n",
    "    \n",
    "    c_clean.imshow(clean_img(d_row['masks']))\n",
    "    c_clean.axis('off')\n",
    "    c_clean.set_title('Clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check RLE\n",
    "Check that our approach for RLE encoding (stolen from [here](https://www.kaggle.com/rakhlin/fast-run-length-encoding-python)) works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import label # label regions\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b+1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "def prob_to_rles(x, cut_off = 0.5):\n",
    "    lab_img = label(x>cut_off)\n",
    "    if lab_img.max()<1:\n",
    "        lab_img[0,0] = 1 # ensure at least one prediction per image\n",
    "    for i in range(1, lab_img.max()+1):\n",
    "        yield rle_encoding(lab_img==i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the RLEs for a Train Image¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_rle_row = next(train_img_df.tail(5).iterrows()) \n",
    "train_row_rles = list(prob_to_rles(train_rle_row['masks']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the RLEs from the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_rles = train_labels.query('ImageId==\"{ImageId}\"'.format(**train_rle_row))['EncodedPixels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check\n",
    "Since we made some simplifications, we don't expect everything to be perfect, but pretty close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match, mismatch = 0, 0\n",
    "for img_rle, train_rle in zip(sorted(train_row_rles, key = lambda x: x[0]), \n",
    "                             sorted(tl_rles, key = lambda x: x[0])):\n",
    "    for i_x, i_y in zip(img_rle, train_rle):\n",
    "        if i_x == i_y:\n",
    "            match += 1\n",
    "        else:\n",
    "            mismatch += 1\n",
    "print('Matches: %d, Mismatches: %d, Accuracy: %2.1f%%' % (match, mismatch, 100.0*match/(match+mismatch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate RLE for all the masks\n",
    "Here we generate the RLE for all the masks and output the the results to a table. We use a few morphological operations to clean up the images before submission since they can be very messy (remove single pixels, connect nearby regions, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_df['rles'] = test_img_df['masks'].map(clean_img).map(lambda x: list(prob_to_rles(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pred_list = []\n",
    "for _, c_row in test_img_df.iterrows():\n",
    "    for c_rle in c_row['rles']:\n",
    "        out_pred_list+=[dict(ImageId=c_row['ImageId'], \n",
    "                             EncodedPixels = ' '.join(np.array(c_rle).astype(str)))]\n",
    "out_pred_df = pd.DataFrame(out_pred_list)\n",
    "print(out_pred_df.shape[0], 'regions found for', test_img_df.shape[0], 'images')\n",
    "out_pred_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pred_df[['ImageId', 'EncodedPixels']].to_csv('predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
